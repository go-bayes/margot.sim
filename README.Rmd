---
output:
  github_document:
    toc: false     # or whatever other github_document options you need
citation_package: biblatex
bibliography: '/Users/joseph/GIT/templates/bib/references.bib'
---
<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# margot.sim

<!-- badges: start -->
[![Lifecycle: experimental](https://img.shields.io/badge/lifecycle-experimental-orange.svg)](https://lifecycle.r-lib.org/articles/stages.html#experimental)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Codecov test coverage](https://codecov.io/gh/go-bayes/margot.sim/graph/badge.svg)](https://app.codecov.io/gh/go-bayes/margot.sim)
[![GitHub stars](https://img.shields.io/github/stars/go-bayes/margot.sim?style=social)](https://github.com/go-bayes/margot.sim)
<!-- CRAN badges: will activate after CRAN release
[![CRAN status](https://www.r-pkg.org/badges/version/margot.sim)](https://CRAN.R-project.org/package=margot.sim)
[![Downloads](https://cranlogs.r-pkg.org/badges/margot.sim)](https://cran.r-project.org/package=margot.sim)
[![Total Downloads](https://cranlogs.r-pkg.org/badges/grand-total/margot.sim)](https://cran.r-project.org/package=margot.sim)
-->
[![R-CMD-check](https://github.com/go-bayes/margot.sim/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/go-bayes/margot.sim/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

R package for simulating longitudinal data with realistic observational shadows (measurement error, missingness, selection bias) and evaluating causal inference methods via Monte Carlo simulation.

## Installation


You can install the development version of margot.sim from [GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("go-bayes/margot.sim")
```

## What's New in v0.1.2

- **6 new comprehensive vignettes** covering shift interventions, censoring, heterogeneous effects, and practical workflows
- **Shadow bias comparison framework** for evaluating how observational distortions affect causal estimates
- **Transport weights integration** for generalizing from samples to target populations
- **Enhanced documentation** with complete examples for all major functions

## Overview

margot.sim extends the `margot` package with:

1. **Shadowing Framework** - Apply observational distortions (measurement error, missingness, selection bias)
2. **Monte Carlo Framework** - Systematically evaluate statistical estimators  
3. **Flexible Distributions** - Specify non-normal distributions
4. **Integrated Workflows** - Complete simulation studies

## Quick Start

```{r example, eval=FALSE}
library(margot.sim)

# Generate data with measurement error
shadow <- create_shadow(
  type = "measurement_error",
  params = list(
    variables = c("t1_l", "t2_l"),
    error_type = "classical",
    sigma = 0.5
  )
)

dat <- margot_simulate(
  n = 1000,
  waves = 3,
  shadows = shadow
)

# Run Monte Carlo evaluation
mc_results <- margot_monte_carlo(
  n_reps = 100,
  n_per_rep = 500,
  dgp_params = list(waves = 2),
  shadows = list(shadow),
  estimator_fn = function(data) {
    fit <- lm(t3_y ~ t2_a + t1_l + b1, data = data)
    list(
      estimate = coef(fit)["t2_a"],
      se = sqrt(diag(vcov(fit)))["t2_a"]
    )
  }
)

print(mc_results)
```

## Shadowing Framework

### Available Shadow Types

**Measurement Error:**
- Classical error (continuous variables)
- Misclassification (binary variables) 
- Differential error
- Dichotomization
- Correlated errors

```{r measurement-error, eval=FALSE}
# Classical measurement error
me_shadow <- create_shadow(
  type = "measurement_error",
  params = list(
    variables = "t1_l",
    error_type = "classical",
    sigma = 0.5
  )
)

# Misclassification for binary variables
misclass_shadow <- create_shadow(
  type = "measurement_error", 
  params = list(
    variables = "t1_a",
    error_type = "misclassification",
    sensitivity = 0.85,  # P(observed=1|true=1)
    specificity = 0.90   # P(observed=0|true=0)
  )
)
```

**Missing Data:**
```{r missingness, eval=FALSE}
# Item-level missingness
miss_shadow <- create_item_missingness_shadow(
  variables = c("t1_l", "t2_l"),
  missing_rate = 0.2,
  missing_mechanism = "MAR",
  dependent_vars = "b1"
)
```

**Selection Bias:**
```{r selection, eval=FALSE}
# Positivity violations
pos_shadow <- create_positivity_shadow(
  exposure_var = "t1_a",
  filter_fn = function(data) {
    # Treatment only possible if risk score <= 2
    data$b1 + data$b2 <= 2
  }
)
```

### Combining Shadows

```{r combining, eval=FALSE}
# Apply multiple shadows
shadows <- list(me_shadow, miss_shadow)
dat <- margot_simulate(n = 1000, waves = 3, shadows = shadows)

# Or apply post-hoc
shadowed_data <- apply_shadows(dat, shadows)
```

## Monte Carlo Framework

Evaluate estimator performance under various conditions:

```{r monte-carlo, eval=FALSE}
# Define estimator
ipw_estimator <- function(data) {
  # Fit propensity score model
  ps_model <- glm(t1_a ~ b1 + b2 + t0_l, 
                  data = data, 
                  family = binomial)
  ps <- predict(ps_model, type = "response")
  
  # Calculate weights
  weights <- ifelse(data$t1_a == 1, 1/ps, 1/(1-ps))
  
  # Outcome model
  fit <- lm(t2_y ~ t1_a, weights = weights, data = data)
  
  list(
    estimate = coef(fit)["t1_a"],
    se = sqrt(diag(vcov(fit)))["t1_a"],
    converged = TRUE
  )
}

# Run simulation
results <- margot_monte_carlo(
  n_reps = 500,
  n_per_rep = 1000,
  dgp_params = list(
    waves = 2,
    params = list(a_lag_y_coef = 0.3)  # True effect
  ),
  shadows = list(me_shadow, miss_shadow),
  estimator_fn = ipw_estimator,
  truth_fn = function(data) 0.3,
  parallel = TRUE,
  n_cores = 4
)

# View results
print(results)
plot(results, type = "histogram")
```

### Performance Metrics

The framework automatically calculates:
- Bias and relative bias
- Variance and MSE
- Coverage of confidence intervals
- Convergence rates
- Sample size retention

## Complete Example

```{r complete-example, eval=FALSE}
# Compare estimators under measurement error
comparison <- example_measurement_error_comparison()

# Full workflow demonstration
results <- example_complete_workflow()
```

## Advanced Usage

### Custom Shadows

Create your own shadow types:

```{r custom-shadow, eval=FALSE}
# Define apply method
apply_shadow.my_custom_shadow <- function(data, shadow, ...) {
  # Your shadowing logic here
  data
}

# Use it
shadow <- structure(
  list(type = "my_custom", params = list(...)),
  class = c("my_custom_shadow", "margot_shadow")
)
```

### Flexible Distributions

```{r distributions, eval=FALSE}
# Non-normal baseline
gamma_dist <- create_distribution(
  "gamma",
  params = list(shape = 2, rate = 1)
)

# Use in simulation
dat <- margot_simulate_flex(
  n = 1000,
  distributions = list(baseline = gamma_dist)
)
```

## Documentation

For detailed documentation, see:

```{r help, eval=FALSE}
# Package documentation
?margot.sim

# Key functions
?create_shadow
?margot_monte_carlo
?margot_simulate
?simulate_ate_data_with_weights

# Vignettes
vignette("basic-simulation", package = "margot.sim")
vignette("applying-shadows", package = "margot.sim") 
vignette("monte-carlo-simple", package = "margot.sim")
vignette("shift-interventions", package = "margot.sim")
vignette("shift-weights", package = "margot.sim")
vignette("censoring-effect-mod", package = "margot.sim")
vignette("heterogeneous-effects", package = "margot.sim")
vignette("advanced-shift-interventions", package = "margot.sim")
vignette("misclassification-bias", package = "margot.sim")
vignette("practical-workflow", package = "margot.sim")
vignette("transport-weights-shadows", package = "margot.sim")
```

## Transport Weights Example

margot.sim supports transportability analyses where you need to generalize from a study sample to a target population:

```{r transport-example, eval=FALSE}
# Simulate data where effect modifier distribution differs
# between sample (10% elderly) and population (50% elderly)
data <- simulate_ate_data_with_weights(
  n_sample = 1000,
  p_z_sample = 0.1,      # 10% elderly in sample
  p_z_population = 0.5,   # 50% elderly in population  
  beta_a = 1,            # base treatment effect
  beta_az = 2,           # treatment works better in elderly
  seed = 2025
)

# Compare effects
sample_ate <- with(data$sample_data,
  mean(y_sample[a_sample == 1]) - mean(y_sample[a_sample == 0]))

weighted_ate <- with(data$sample_data, {
  weighted.mean(y_sample[a_sample == 1], weights[a_sample == 1]) -
  weighted.mean(y_sample[a_sample == 0], weights[a_sample == 0])
})

cat("Sample ATE:", round(sample_ate, 2), "\n")
cat("Population ATE (weighted):", round(weighted_ate, 2), "\n")
```

## Contributing

Contributions are welcome. Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Submit a pull request


## License

MIT License

## Citation

If you use margot.sim in your research, please cite:

## Why do we need a beefed-up simulator?

Real studies seldom hand us tidy, truth-telling data. Instead we get scratches and dents:
*	Missing answers.  People skip survey items or drop out of follow-ups.
*	Blurred measurements.  Blood-pressure cuffs mis-read, self-reports round up or down.
*	Uneven treatment chances.  Some groups are far more (or far less) likely to receive the exposure we're studying, making "apples-with-apples" comparisons tricky.

Traditional toy simulations gloss over these hassles, so a method that looks brilliant on paper can wilt in practice. Here we:

1.	Mirror the messiness.  Simulating truth and then adding "shadows" let us layer on realistic missing data, measurement error, and selection bias.
That means our virtual datasets behave more like the ones sitting on an analyst's desk.
2.	Stress-test causal tools.  We can ask, "Does this method still give the right answer if half the blood-pressure readings are off by 5 mmHg? If drop-out is twice as common among the treated?"  Running thousands of such scenarios in silico is faster, cheaper, and safer than discovering the problem after a costly field study.
3.	Keep truth separate from distortion.  By generating the "clean" data first and then adding each shadow, we always know the ground truth.  That makes it painless to check how far an estimator strays once the smoke and mirrors appear.
4.	Plug in any causal question so that users can tailor simulations to the exact claim they plan to make.
5.	Build confidence.  Showing that a method works under sharp, realistic testsâ€”especially can gives us (and journal referees) fewer reasons to worry.


The `margot.sim` package provides a simulation framework to quantify how statistical estimators perform when the data they are fed are *shadows* of a true causal process. This concept nods to Plato's "Allegory of the Cave," where prisoners mistake shadows cast on walls for reality [@bloom1968republic]. `margot.sim` gives us access to simulated ground truth so that we can evaluate how far our data and modelling choices can mislead us. The framework's power comes from its consistent architecture that maintains **a clean separation between the true data-generating process and the distorted data that investigators observe**, allowing for principled evaluation of statistical methods in the face of data limitations that inevitably arise in science [@bulbulia2024wierd].

## References

```{r citation, eval=FALSE}
citation("margot.sim")
```
